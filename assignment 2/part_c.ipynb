{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d514dc",
   "metadata": {},
   "source": [
    "# Testing against other software libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6c81c",
   "metadata": {},
   "source": [
    "### Check of vectors equal from vector and batch implementation\n",
    "Have not had time to check this properly yet\n",
    "\n",
    "\n",
    "\n",
    "Even with the same seed, create_layers and create_layers_batch generate different weight tensors:\n",
    "\n",
    "Vector uses W_vec with shape (out_dim, in_dim).\n",
    "Batch uses W_bat with shape (in_dim, out_dim).\n",
    "\n",
    "Resetting the RNG and sampling for a different shape produces different matrices (not transposes of each other). So you’re comparing gradients from different networks.\n",
    "\n",
    "✅ Fix: Create one set of parameters (e.g., vector convention), then convert to the other convention by transposing each W. Do the same conversion for gradients when comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_batch_layers(layers_vec):\n",
    "    \"\"\"\n",
    "    (W_vec: out x in, b: out) -> (W_bat: in x out, b: out)\n",
    "    \"\"\"\n",
    "    return [(Wv.T.copy(), bv.copy()) for (Wv, bv) in layers_vec]\n",
    "\n",
    "def batch_to_vec_grads(grads_bat):\n",
    "    \"\"\"\n",
    "    (dW_bat: in x out, db: out) -> (dW_vec: out x in, db: out)\n",
    "    \"\"\"\n",
    "    return [(dWb.T.copy(), db.copy()) for (dWb, db) in grads_bat]\n",
    "\n",
    "\n",
    "\n",
    "# ---- Setup ----\n",
    "network_input_size = 2\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "\n",
    "# Same single example in both forms\n",
    "np.random.seed(2024)\n",
    "x_batch = np.random.rand(1, network_input_size)\n",
    "np.random.seed(2024)\n",
    "target_batch = np.random.rand(1, 4)\n",
    "x_vector = x_batch[0]\n",
    "target_vector = target_batch[0]\n",
    "\n",
    "# Create ONE parameter set in vector convention (assumed: W_vec is (out, in))\n",
    "np.random.seed(2024)\n",
    "layers_vector = create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "# Convert that SAME network to batch convention\n",
    "layers_batch = vec_to_batch_layers(layers_vector)\n",
    "\n",
    "# ---- Run ----\n",
    "vector_grads = backpropagation(x_vector, layers_vector, activation_funcs, target_vector, activation_ders)\n",
    "batch_grads  = backpropagation_batch(x_batch, layers_batch, activation_funcs, target_batch, activation_ders)\n",
    "\n",
    "print(vector_grads)\n",
    "print(batch_grads)\n",
    "\n",
    "# Convert batch grads back to vector orientation\n",
    "batch_grads_as_vector = batch_to_vec_grads(batch_grads)\n",
    "\n",
    "# ---- Compare ----\n",
    "for i, ((dW_v, db_v), (dW_bv, db_bv)) in enumerate(zip(vector_grads, batch_grads_as_vector)):\n",
    "    print(f\"Layer {i}: dW match ->\", np.allclose(dW_v, dW_bv, atol=1e-8))\n",
    "    print(f\"Layer {i}: db match ->\", np.allclose(db_v, db_bv, atol=1e-8))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
