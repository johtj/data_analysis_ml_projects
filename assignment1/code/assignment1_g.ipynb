{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd342d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import common modules\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import own modules\n",
    "from source.matrix_creation import polynomial_features,scale_features_by_intercept_use\n",
    "from source.plotting_exploration import explore_iterations_GD_methods_ridge,explore_iterations_GD_methods_OLS\n",
    "from source.plotting_exploration import explore_n_epochs_stochasticGD_ridge,explore_n_epochs_stochasticGD_OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99e5fd",
   "metadata": {},
   "source": [
    "## Notebook for assignment 1 parts c,d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205df64d",
   "metadata": {},
   "source": [
    "Generating analysis plots for assignement 1 parts c) and d) exploring Gradient descent,. \n",
    "\n",
    "* Functions for the main methods can be found in source/main_methods.py\n",
    "* Functions generating and scaling the feature matrix can be found in source/matrix_creation.py \n",
    "* Functions for calculating errors can be found in source/errors.py\n",
    "* Functions for plotting and exploration can be found in source/plotting_exploration.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed850d2",
   "metadata": {},
   "source": [
    "### Setup for constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54339ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runge's function - parameters to explore\n",
    "n_datapoints = 200\n",
    "standard_deviation = 0.1 # for noise\n",
    "p = 15 # polynomial degree\n",
    "\n",
    "lambda_range = (-1,-5) # range of lambda values for np.log\n",
    "lambda_n = 20 # number lambda values to explore\n",
    "lambdas_start = np.logspace(lambda_range[0],lambda_range[1],lambda_n) # lambdas generated in logspace for learning rate\n",
    "\n",
    "# Grid search\n",
    "etas = [0.001, 0.005, 0.01, 0.05, 0.1] # gradient descent parameters\n",
    "\n",
    "# tolerance criteria for gradient descent methods\n",
    "tolerance = 1e-6\n",
    "max_iterations = 1000\n",
    "\n",
    "use_intercept = True \n",
    "create_plots = False\n",
    "verbose_bool = False\n",
    "\n",
    "np.random.seed(250)  # ensure reproducibility numpy\n",
    "random_state_int = 42   # ensure reproducibility train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e861d9",
   "metadata": {},
   "source": [
    "### Generate Runge's function data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5150bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating data without noise\n",
    "x = np.linspace(-1, 1, num=n_datapoints)\n",
    "y = 1 / (1 + 25 * x**2)\n",
    "\n",
    "# generating data with noise\n",
    "x_noise = x\n",
    "y_noise = 1 / (1 + 25 * x_noise**2) + np.random.normal(0, standard_deviation, n_datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d961a",
   "metadata": {},
   "source": [
    "### Generate feature matrix, scale and split into training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabda8b",
   "metadata": {},
   "source": [
    "#### No noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b92284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating design matrix with polynomial features: p\n",
    "X = polynomial_features(x, p,intercept=use_intercept) # intercept=True gives intercept column = 0 in standard scaler if intercept is True, and hence division by 0. \n",
    "\n",
    "# split x for plotting\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random_state_int)\n",
    "# test and train dataset, and scaling of X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state_int)\n",
    "X_train_scaled, X_test_scaled, X_train_mean, X_train_std = scale_features_by_intercept_use(X_train, X_test, use_intercept)\n",
    "# scaling of y_train and y_test\n",
    "y_train_scaled, y_test_scaled, y_train_mean, y_train_std = standard_scaler(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c8a8e",
   "metadata": {},
   "source": [
    "#### With noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating design matrix with polynomial features: p\n",
    "X_noise = polynomial_features(x_noise, p,intercept=use_intercept) # intercept=True gives intercept column = 0 in standard scaler if intercept is True, and hence division by 0. Leaving intercept out since Ridge regression handles this\n",
    "\n",
    "# test and train dataset, and scaling of X_train and X_test\n",
    "x_train_noise, x_test_noise, y_train_noise, y_test_noise = train_test_split(x_noise, y_noise, test_size=0.2, random_state=random_state_int)\n",
    "X_train_noise, X_test_noise, y_train_noise, y_test_noise = train_test_split(X_noise, y_noise, test_size=0.2, random_state = random_state_int)\n",
    "X_train_scaled_noise, X_test_scaled_noise, X_mean, X_std = scale_features_by_intercept_use(X_train_noise, X_test_noise, use_intercept)\n",
    "# scaling of y_train and y_test\n",
    "y_train_scaled_noise, y_test_scaled_noise, y_train_mean_noise, y_train_std_noise = standard_scaler(y_train_noise, y_test_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874e918",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
