{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30bbf5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "from matrix_creation import polynomial_features, standard_scaler, scale_features_by_intercept_use\n",
    "from main_methods import OLS_parameters, Ridge_parameters, gradient_descent_OLS, gradient_descent_ridge, sklearn_lasso_regression, rescale_theta_intercept, predict_y, rescale_y\n",
    "from errors import MSE,R2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858e72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_exploration import plot_mse,plot_r2, explore_lambda, explore_polynomial_degree, lasso_grid_search, plot_heatmap_lasso, plot_theta_by_polynomials, plot_xy_xynoise_ypredicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32822adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runge's function - parameters to explore\n",
    "\n",
    "n_datapoints = 100 # changed and ran code with different values. Could have been implemented as a list and looped over, but regarded as not necessary.\n",
    "standard_deviation = 0.1 # for noise, should we play around with this values as well in analysis?\n",
    "p = 10 # polynomial degree\n",
    "\n",
    "lambda_range = (-1,-5) # range of lambda values for np.log\n",
    "lambda_n = 20 # number lambda values to explore\n",
    "lambdas_start = np.logspace(lambda_range[0],lambda_range[1],lambda_n) # lambdas generated in logspace for learning rate\n",
    "\n",
    "# Grid search\n",
    "etas = [0.001, 0.005, 0.01, 0.05, 0.1] # gradient descent parameters\n",
    "\n",
    "# tolerance criteria for gradient descent methods\n",
    "tolerance = 1e-6\n",
    "max_iterations = 1000\n",
    "\n",
    "# changed to variable to easily switch between True and False for fit_intercept\n",
    "use_intercept = True \n",
    "\n",
    "# create plots or not\n",
    "create_plots = False\n",
    "\n",
    "verbose_bool = False\n",
    "\n",
    "np.random.seed(250)  # ensure reproducibility numpy\n",
    "random_state_int = 42   # ensure reproducibility train_test_split\n",
    "\n",
    "# generating data without noise\n",
    "x = np.linspace(-1, 1, num=n_datapoints)\n",
    "y = 1 / (1 + 25 * x**2)\n",
    "\n",
    "# generating data with noise\n",
    "x_noise = x\n",
    "y_noise = 1 / (1 + 25 * x_noise**2) + np.random.normal(0, standard_deviation, n_datapoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3404950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline with OLS for comparison of other regressions\n",
      "sklearn OLS not scaled. Coef: [ 0.00000000e+00  8.31699774e-03 -1.01804023e+01 -3.23517869e-01\n",
      "  4.85995012e+01  1.80668138e+00 -1.05812907e+02 -3.19367667e+00\n",
      "  1.04932103e+02  1.75437381e+00 -3.85045488e+01], intercept: 0.9047537487297413 \n",
      "sklearn OLS scaled. Coef: [ 0.00000000e+00  1.68017291e-02 -1.09385594e+01 -4.42113110e-01\n",
      "  4.72980648e+01  2.00591523e+00 -9.28221321e+01 -3.07401111e+00\n",
      "  8.43114252e+01  1.51490714e+00 -2.87255159e+01], intercept: -6.0409431386416235e-15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brumor\\AppData\\Local\\miniconda3\\envs\\4155\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.294e-01, tolerance: 8.000e-05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Runge's function analysis\n",
    "##################################################\n",
    "\n",
    "\n",
    "# OLS as function of polynomial degree\n",
    "# No noise\n",
    "# creating design matrix with polynomial features: p\n",
    "X = polynomial_features(x, p,intercept=use_intercept) # intercept=True gives intercept column = 0 in standard scaler if intercept is True, and hence division by 0. \n",
    "\n",
    "# split x for plotting\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=random_state_int)\n",
    "# test and train dataset, and scaling of X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state_int)\n",
    "X_train_scaled, X_test_scaled, X_train_mean, X_train_std = scale_features_by_intercept_use(X_train, X_test, use_intercept)\n",
    "# scaling of y_train and y_test\n",
    "y_train_scaled, y_test_scaled, y_train_mean, y_train_std = standard_scaler(y_train, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcualte OLS with sklearn as baseline values of intercept and theta\n",
    "# Compare other regression results to this\n",
    "ols_model_not_scaled = LinearRegression(fit_intercept=use_intercept)\n",
    "ols_model_scaled = LinearRegression(fit_intercept=use_intercept)\n",
    "ols_sklearn_not_scaled = ols_model_not_scaled.fit(X_train, y_train)\n",
    "ols_sklearn_scaled = ols_model_scaled.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"Baseline with OLS for comparison of other regressions\")\n",
    "print(f\"sklearn OLS not scaled. Coef: {ols_sklearn_not_scaled.coef_}, intercept: {ols_sklearn_not_scaled.intercept_} \")\n",
    "print(f\"sklearn OLS scaled. Coef: {ols_sklearn_scaled.coef_}, intercept: {ols_sklearn_scaled.intercept_}\")\n",
    "rescaled_coef, rescaled_intercept = rescale_theta_intercept(ols_sklearn_scaled.coef_[1:], ols_sklearn_scaled.intercept_, y_train_std, y_train_mean, X_train_std, X_train_mean, verbose=verbose_bool)\n",
    "print('\\n\\n\\n\\n')\n",
    "\n",
    "################ Regression without noise\n",
    "# scaled data for features as input \n",
    "polynomial_degree, mse_train, mse_test, r2_train, r2_test, thetas_ols_noise = explore_polynomial_degree(X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, p, use_intercept=use_intercept, verbose=verbose_bool)\n",
    "# rescaled coef and intercept as qc\n",
    "rescaled_coef_ols, rescaled_intercept_ols = rescale_theta_intercept(thetas_ols_noise[-1][1:], thetas_ols_noise[-1][0], y_train_std, y_train_mean, X_train_std, X_train_mean, verbose=verbose_bool)\n",
    "y_predicted_scaled_ols = predict_y(X_test_scaled[:, 1:], rescaled_coef_ols)\n",
    "y_predicted_rescaled_ols = rescale_y(y_predicted_scaled_ols, y_train_std, y_train_mean)\n",
    "\n",
    "if create_plots:\n",
    "    plot_mse(\"OLS\", p, n_datapoints, polynomial_degree, mse_train, mse_test, noise=False)\n",
    "    plot_r2(\"OLS\", p, n_datapoints, polynomial_degree, r2_train, r2_test, noise=False)\n",
    "\n",
    "\n",
    "# Explore lambdas with Ridge regression - no noise\n",
    "mse_train_ridge, mse_test_ridge, r2_train_ridge, r2_test_ridge, theta_ridge = explore_lambda(X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, lambdas_start, verbose=verbose_bool)\n",
    "# rescaled coef and intercept as qc\n",
    "rescaled_coef_ridge, rescaled_intercept_ridge = rescale_theta_intercept(theta_ridge[-1][1:], theta_ridge[-1][0], y_train_std, y_train_mean, X_train_std, X_train_mean, verbose=verbose_bool)\n",
    "y_predicted_scaled_ridge = predict_y(X_test_scaled[:, 1:], rescaled_coef_ridge)\n",
    "y_predicted_rescaled_ridge = rescale_y(y_predicted_scaled_ridge, y_train_std, y_train_mean)\n",
    "\n",
    "if create_plots: #MSE and R2 with no noise\n",
    "    plot_mse(\"Ridge\", p, n_datapoints, lambdas_start, mse_train_ridge, mse_test_ridge, noise=False)\n",
    "    plot_r2(\"Ridge\", p, n_datapoints, lambdas_start, r2_train_ridge, r2_test_ridge, noise=False)\n",
    "\n",
    "\n",
    "################# Regression with noise\n",
    "# creating design matrix with polynomial features: p\n",
    "X_noise = polynomial_features(x_noise, p,intercept=use_intercept) # intercept=True gives intercept column = 0 in standard scaler if intercept is True, and hence division by 0. Leaving intercept out since Ridge regression handles this\n",
    "\n",
    "# test and train dataset, and scaling of X_train and X_test\n",
    "x_train_noise, x_test_noise, y_train_noise, y_test_noise = train_test_split(x_noise, y_noise, test_size=0.2, random_state=random_state_int)\n",
    "X_train_noise, X_test_noise, y_train_noise, y_test_noise = train_test_split(X_noise, y_noise, test_size=0.2, random_state = random_state_int)\n",
    "X_train_scaled_noise, X_test_scaled_noise, X_mean, X_std = scale_features_by_intercept_use(X_train_noise, X_test_noise, use_intercept)\n",
    "# scaling of y_train and y_test\n",
    "y_train_scaled_noise, y_test_scaled_noise, y_train_mean_noise, y_train_std_noise = standard_scaler(y_train_noise, y_test_noise)\n",
    "\n",
    "### OLS\n",
    "# scaled data for features as input\n",
    "polynomial_degree, mse_train, mse_test, r2_train, r2_test, thetas_ols_noise = explore_polynomial_degree(X_train_scaled_noise, X_test_scaled_noise, y_train_scaled_noise, y_test_scaled_noise, p, use_intercept=use_intercept, verbose=verbose_bool)\n",
    "# rescaled coef and intercept as qc, calculated rescaled y_predict\n",
    "rescaled_coef_ols_noise, rescaled_intercept_ols_noise = rescale_theta_intercept(thetas_ols_noise[-1][1:], thetas_ols_noise[-1][0], y_train_std, y_train_mean, X_train_std, X_train_mean, verbose=verbose_bool)\n",
    "y_predicted_scaled_ols_noise = predict_y(X_test_scaled_noise[:, 1:], rescaled_coef_ols_noise)\n",
    "y_predicted_rescaled_ols_noise = rescale_y(y_predicted_scaled_ols_noise, y_train_std, y_train_mean)\n",
    "if create_plots: #MSE, R2 and function with noise\n",
    "    plot_mse(\"OLS\", p, n_datapoints, polynomial_degree, mse_train, mse_test, noise=True)\n",
    "    plot_r2(\"OLS\", p, n_datapoints, polynomial_degree, r2_train, r2_test, noise=True)\n",
    "    plot_xy_xynoise_ypredicted(x, y, x_train_noise, y_train_noise, y_predicted_rescaled_ols_noise, x_test, n_datapoints, \"OLS\", p, True, lambda_n, 0, etas, max_iterations)\n",
    "\n",
    "# Plot theta values as function if polynomial degree\n",
    "if create_plots: plot_theta_by_polynomials(thetas_ols_noise, p, n_datapoints)\n",
    "\n",
    "# Explore lambdas with Ridge regression - noise\n",
    "mse_train_ridge, mse_test_ridge, r2_train_ridge, r2_test_ridge, thetas_ridge_noise = explore_lambda(X_train_scaled_noise, X_test_scaled_noise, y_train_scaled_noise, y_test_scaled_noise, lambdas_start, verbose=verbose_bool)\n",
    "\n",
    "if create_plots:\n",
    "    # for plotting ridge dependent on different lambda values, rescaled coef and intercept also as qc\n",
    "    for i, theta in enumerate(theta_ridge):\n",
    "        rescaled_coef_ridge_noise, rescaled_intercept_ridge_noise = rescale_theta_intercept(theta[1:], theta[0], y_train_std, y_train_mean, X_train_std, X_train_mean, verbose=verbose_bool)\n",
    "        y_predicted_scaled_ridge_noise = predict_y(X_test_scaled[:, 1:], rescaled_coef_ridge_noise)\n",
    "        y_predicted_rescaled_ridge_noise = rescale_y(y_predicted_scaled_ridge_noise, y_train_std, y_train_mean)\n",
    "        plot_xy_xynoise_ypredicted(x, y, x_train_noise, y_train_noise, y_predicted_rescaled_ridge_noise, x_test, n_datapoints, \"Ridge\", p, False, lambda_n, lambdas_start[i], 0, max_iterations)\n",
    "\n",
    "if create_plots:\n",
    "    plot_mse(\"Ridge\", p, n_datapoints, lambdas_start, mse_train_ridge, mse_test_ridge, noise=True)\n",
    "    plot_r2(\"Ridge\", p, n_datapoints, lambdas_start, r2_train_ridge, r2_test_ridge, noise=True)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Explore Lasso regression as function of lambdas and learning rate ----- Only used withot noise\n",
    "lasso_grid, lasso_mse_train, lasso_mse_test, lasso_r2_train, lasso_r2_test, lasso_mse_values, lasso_r2_values = lasso_grid_search(X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, lambdas_start, etas, tolerance, max_iterations, use_intercept, verbose=verbose_bool)\n",
    "# rescaled coef and intercept as qc\n",
    "rescaled_coef_lasso_noise, rescaled_intercept_lasso_noise = rescale_theta_intercept(lasso_grid['coef'][1:], lasso_grid['intercept'], y_train_std, y_train_mean, X_train_std, X_train_mean, verbose=verbose_bool)\n",
    "y_predicted_scaled_lasso_noise = predict_y(X_test_scaled_noise[:, 1:], rescaled_coef_lasso_noise)\n",
    "y_predicted_rescaled_lasso_noise = rescale_y(y_predicted_scaled_lasso_noise, y_train_std, y_train_mean)\n",
    "if create_plots:\n",
    "    plot_xy_xynoise_ypredicted(x, y, x_train_noise, y_train_noise, y_predicted_rescaled_lasso_noise, x_test, n_datapoints, \"Lasso\", p, True, lambda_n, 0, etas, max_iterations)\n",
    "    plot_heatmap_lasso(\"MSE\", lasso_mse_values, lambdas_start, etas, p, n_datapoints, max_iterations)\n",
    "    plot_heatmap_lasso(\"R2\", lasso_r2_values, lambdas_start, etas, p, n_datapoints, max_iterations)\n",
    "\n",
    "# compare with lasso sklearn - use best alpha value from own implementation and calculation of lasso\n",
    "if verbose_bool: print('lasso - not scaled')\n",
    "sklearn_lasso_regression(X_train, y_train, lasso_grid['lambda'], use_intercept, max_iterations, tolerance, verbose=verbose_bool)\n",
    "if verbose_bool: print('lasso - scaled')\n",
    "sklearn_lasso_regression(X_train_scaled, y_train_scaled, lasso_grid['lambda'], use_intercept, max_iterations, tolerance, verbose=verbose_bool)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613408f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4155 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
