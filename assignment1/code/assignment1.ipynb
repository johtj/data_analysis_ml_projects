{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bbf5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from matrix_creation import polynomial_features, standard_scaler,scale_features_by_intercept_use\n",
    "from main_methods import OLS_parameters, Ridge_parameters, gradient_descent_OLS, gradient_descent_ridge, sklearn_lasso_regression\n",
    "from errors import MSE,R2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858e72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_exploration import plot_mse,plot_r2,explore_lambda,explore_polynomial_degree, lasso_grid_search, plot_heatmap_lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32822adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runge's function - parameters to explore\n",
    "\n",
    "n_datapoints = 50 # changed and ran code with different values. Could have been implemented as a list and looped over, but regarded as not necessary.\n",
    "standard_deviation = 0.1 # for noise, should we play around with this values as well in analysis?\n",
    "p = 15 # polynomial degree\n",
    "\n",
    "lambda_range = (-1,-5) # range of lambda values for np.log\n",
    "lambda_n = 50 # number lambda values to explore\n",
    "lambdas_start = np.logspace(lambda_range[0],lambda_range[1],lambda_n) # lambdas generated in logspace for learning rate\n",
    "\n",
    "# Grid search\n",
    "etas = [0.001, 0.005, 0.01, 0.05, 0.1] # gradient descent parameters\n",
    "\n",
    "# tolerance criteria for gradient descent methods\n",
    "tolerance = 1e-6\n",
    "max_iterations = 1000\n",
    "\n",
    "# changed to variable to easily switch between True and False for fit_intercept\n",
    "use_intercept = True \n",
    "\n",
    "# create plots or not\n",
    "create_plots = False\n",
    "\n",
    "np.random.seed(250)  # ensure reproducibility\n",
    "\n",
    "# generating data without noise\n",
    "x = np.linspace(-1, 1, num=n_datapoints)\n",
    "y = 1 / (1 + 25 * x**2)\n",
    "\n",
    "# generating data with noise\n",
    "x_noise = np.linspace(-1, 1, num=n_datapoints) + np.random.normal(0, standard_deviation, n_datapoints)\n",
    "y_noise = 1 / (1 + 25 * x_noise**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3404950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1e-05, MSE_train_lasso: 0.02338258151355871, MSE_test_lasso: 0.027781854953036107\n",
      "Lambda: 1e-05, R2_train_lasso: 0.7081184576760865, R2_test_lsso: 0.6399348624203886\n",
      "Own implementation Lasso\n",
      "0.0010985411419875584\n",
      "0.1\n",
      "0.027781854953036107\n",
      "[ 0.28567014  0.00436228 -1.23931283 -0.01615816  0.15460858 -0.00179762\n",
      "  0.28798858  0.          0.21948506  0.          0.11415675  0.\n",
      "  0.01503896  0.         -0.02175521  0.        ] 0.2870915103941953\n",
      "\n",
      "\n",
      "Sklearn coef: [ 0.00000000e+00  1.70222666e-03 -2.16313291e+00 -0.00000000e+00\n",
      "  1.82337308e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -2.38451504e-01  0.00000000e+00], Sklearn intercept: 0.6420579703744886\n",
      "Sklearn coef: [ 0.  0. -0.  0. -0.  0. -0.  0. -0.  0. -0.  0. -0.  0. -0.  0.], Sklearn intercept: 0.2863630230070264\n"
     ]
    }
   ],
   "source": [
    "# Runge's function analysis\n",
    "##################################################\n",
    "\n",
    "\n",
    "# OLS as function of polynomial degree\n",
    "# No noise\n",
    "# creating design matrix with polynomial features: p\n",
    "X = polynomial_features(x, p,intercept=use_intercept) # intercept=True gives intercept column = 0 in standard scaler if intercept is True, and hence division by 0. \n",
    "\n",
    "# test and train dataset, and scaling of X_train and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train_scaled, X_test_scaled = scale_features_by_intercept_use(X_train, X_test, use_intercept)\n",
    "\n",
    "# scaled data for features as input \n",
    "polynomial_degree, mse_train, mse_test, r2_train, r2_test = explore_polynomial_degree(X_train_scaled, X_test_scaled, y_train, y_test, p, use_intercept=use_intercept)\n",
    "if create_plots:\n",
    "    plot_mse(n_datapoints, polynomial_degree,\"Polynomial degrees\", mse_train, mse_test, noise=False)\n",
    "    plot_r2(n_datapoints, polynomial_degree,\"Polynomial degrees\", r2_train, r2_test, noise=False)\n",
    "\n",
    "lambdas, mse_train_ridge, mse_test_ridge, r2_train_ridge, r2_test_ridge = explore_lambda(X_train_scaled, X_test_scaled, y_train, y_test, lambdas_start)\n",
    "if create_plots:\n",
    "    plot_mse(n_datapoints, lambdas,\"Lambda values\", mse_train_ridge, mse_test_ridge, noise=False)\n",
    "    plot_r2(n_datapoints, lambdas,\"Lambda values\", r2_train_ridge, r2_test_ridge, noise=False)\n",
    "\n",
    "\n",
    "# With noise\n",
    "# creating design matrix with polynomial features: p\n",
    "X_noise = polynomial_features(x_noise, p,intercept=use_intercept) # intercept=True gives intercept column = 0 in standard scaler if intercept is True, and hence division by 0. Leaving intercept out since Ridge regression handles this\n",
    "\n",
    "# test and train dataset, and scaling of X_train and X_test\n",
    "X_train_noise, X_test_noise, y_train_noise, y_test_noise = train_test_split(X_noise, y_noise, test_size=0.2)\n",
    "X_train_scaled_noise, X_test_scaled_noise = scale_features_by_intercept_use(X_train_noise, X_test_noise, use_intercept)\n",
    "\n",
    "# scaled data for features as input \n",
    "polynomial_degree, mse_train, mse_test, r2_train, r2_test = explore_polynomial_degree(X_train_scaled_noise, X_test_scaled_noise, y_train_noise, y_test_noise, p, use_intercept=use_intercept)\n",
    "if create_plots:\n",
    "    plot_mse(n_datapoints, polynomial_degree, \"polynomial degrees\", mse_train, mse_test, noise=True)\n",
    "    plot_r2(n_datapoints, polynomial_degree, \"polynomial degrees\",r2_train, r2_test, noise=True)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Explore lambdas with Ridge regression\n",
    "lambdas, mse_train_ridge, mse_test_ridge, r2_train_ridge, r2_test_ridge = explore_lambda(X_train_scaled_noise, X_test_scaled_noise, y_train_noise, y_test_noise, lambdas_start)\n",
    "if create_plots:\n",
    "    plot_mse(n_datapoints, lambdas,\"Lambda values\", mse_train_ridge, mse_test_ridge, noise=False)\n",
    "    plot_r2(n_datapoints, lambdas,\"Lambda values\", r2_train_ridge, r2_test_ridge, noise=False)\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Explore Lasso regression as function of lambdas and learning rate\n",
    "verbose_lasso = True\n",
    "lasso_grid, lasso_mse_train, lasso_mse_test, lasso_r2_train, lasso_r2_test, mse_values = lasso_grid_search(X_train, X_test, y_train, y_test, lambdas_start, etas, tolerance, max_iterations, use_intercept, verbose=verbose_lasso)\n",
    "if create_plots:\n",
    "    plot_heatmap_lasso(mse_values, lambda_n, etas)\n",
    "# compare with lasso sklearn - use alpha value from own implementation and calculation of lasso, best value\n",
    "#                     --------> use lambda_ or learning rate, or a constant? Unsure about the calculated intercept value\n",
    "sklearn_lasso_regression(X_train, y_train, lasso_grid['lambda'], use_intercept, max_iterations, tolerance, verbose=verbose_lasso)\n",
    "sklearn_lasso_regression(X_train, y_train, lasso_grid['learning_rate'], use_intercept, max_iterations, tolerance, verbose=verbose_lasso) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14f405",
   "metadata": {},
   "source": [
    "# About scaling - information for report\n",
    "\n",
    "See \"machine learning with Python and Scikit-learn - page 119-122\" and https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-standard-scaler-section.\n",
    "From scikit documentation handling of outliers are important when scaling. Standard scaler is not robust to outliers, men our dataset is generated with some noise but no outliers. Standard scaler is hence ok to use, and mean and variance is mentioned. Write arguement for using standard scaler and not normaliazation.\n",
    "\n",
    "From lecture notes: \n",
    "\n",
    "The StandardScaler function in Scikit-Learn ensures that for each feature/predictor we study the mean value is zero and the variance is one (every column in the design/feature matrix). This scaling has the drawback that it does not ensure that we have a particular maximum or minimum in our data set.\n",
    "\n",
    "The Normalizer scales each data point such that the feature vector has a euclidean length of one. In other words, it projects a data point on the circle (or sphere in the case of higher dimensions) with a radius of 1. \n",
    "This means every data point is scaled by a different number (by the inverse of itâ€™s length). This normalization is often used when only the direction (or angle) of the data matters, not the length of the feature vector.\n",
    "\n",
    "The RobustScaler works similarly to the StandardScaler in that it ensures statistical properties for each feature that guarantee that they are on the same scale. \n",
    "However, the RobustScaler uses the median and quartiles, instead of mean and variance. This makes the RobustScaler ignore data points that are very different from the rest (like measurement errors). \n",
    "These odd data points are also called outliers, and might often lead to trouble for other scaling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613408f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4155 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
